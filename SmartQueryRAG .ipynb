{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Required Library Installation\n",
        "# Install LangChain-Pinecone and LangChain-Google-GenAI\n",
        "%pip install -qU langchain-pinecone langchain-google-genai"
      ],
      "metadata": {
        "id": "Xe26aMkiwnum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from google.colab import userdata\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_core.documents import Document\n",
        "from uuid import uuid4\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "b96QkDCswnrM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Pinecone\n",
        "pinecone_api_key = userdata.get('Pinecone')  # Retrieve Pinecone API key from user data\n",
        "pc = Pinecone(api_key=pinecone_api_key)"
      ],
      "metadata": {
        "id": "Fjgp4T1dwnoC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete Existing Index (if applicable)\n",
        "pc.delete_index(index_name)  # Delete the index if it exists\n",
        "print(f\"Index '{index_name}' deleted successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA_mi2kxxfwj",
        "outputId": "9f48345b-72f0-447d-cca9-3d44c21d598d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 'online-rag-project' deleted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"online-rag-project\"  # Specify the index name\n",
        "\n",
        "# Create New Index\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=768,\n",
        "    metric=\"cosine\",\n",
        "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        ")\n"
      ],
      "metadata": {
        "id": "vIrVnJ_nwnk-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the Newly Created Index\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "1-gLksnlwnhr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Google Generative AI Embeddings\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')  # Set the Google API key as an environment variable\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001')"
      ],
      "metadata": {
        "id": "dN8OAAq8wne8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed a Sample Query\n",
        "vector = embeddings.embed_query('muhammad Junaid')\n"
      ],
      "metadata": {
        "id": "yeMP3Kk2wncG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Pinecone Vector Store\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "CKifD2alwnYz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Documents to the Vector Store\n",
        "# Define Sample Documents\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees Fahrenheit.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n"
      ],
      "metadata": {
        "id": "wdv2dp-UwnV0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Unique IDs for the Documents\n",
        "documents = [document_1, document_2, document_3]\n",
        "uuids = [str(uuid4()) for _ in range(len(documents))]"
      ],
      "metadata": {
        "id": "9uni7eFnwnSr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Documents to the Vector Store\n",
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07YK_F_lwnPx",
        "outputId": "01b422b9-9614-477e-81a1-586989c6165a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['26bc3679-f9d8-4211-8599-94022ce072b4',\n",
              " 'a159482c-d5bf-4fab-a553-2b14376f2b83',\n",
              " '77baecd2-ba55-44a2-b7d4-ba1bef97f4b1']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve Documents Using Similarity Search\n",
        "# Search by Content and Filter by Metadata\n",
        "results = vector_store.similarity_search(\n",
        "    'LangeChain provide abstractions to make working with LLMs easy',\n",
        "    k=2,\n",
        "    filter={'source': 'tweet'}\n",
        ")"
      ],
      "metadata": {
        "id": "k_fYls7OwnNB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Retrieved Results\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOHW0mR6wnKW",
        "outputId": "3af66dd2-2c81-4bb6-eddd-b7b3b203d3d5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n",
            "* I had chocolate chip pancakes and scrambled eggs for breakfast this morning. [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve Documents Using Similarity Search with Scores\n",
        "results_with_scores = vector_store.similarity_search_with_score(\n",
        "    \"will it be hot tomorrow\"\n",
        ")"
      ],
      "metadata": {
        "id": "SEwg1v2UwnHJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results_with_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF2z5PzlwnEJ",
        "outputId": "bb79d787-ff41-4bdc-8711-d1ee7fee6532"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(Document(id='a159482c-d5bf-4fab-a553-2b14376f2b83', metadata={'source': 'news'}, page_content='The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees Fahrenheit.'), 0.675364912), (Document(id='26bc3679-f9d8-4211-8599-94022ce072b4', metadata={'source': 'tweet'}, page_content='I had chocolate chip pancakes and scrambled eggs for breakfast this morning.'), 0.493864655), (Document(id='77baecd2-ba55-44a2-b7d4-ba1bef97f4b1', metadata={'source': 'tweet'}, page_content='Building an exciting new project with LangChain - come check it out!'), 0.479618132)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Retrieved Results with Scores\n",
        "for res, score in results_with_scores:\n",
        "    print(f\"* [SIM={score:.3f}] {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKjBmWn6wnBP",
        "outputId": "9d41048f-f668-42bd-c8cf-09ef4f665432"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* [SIM=0.675] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees Fahrenheit. [{'source': 'news'}]\n",
            "* [SIM=0.494] I had chocolate chip pancakes and scrambled eggs for breakfast this morning. [{'source': 'tweet'}]\n",
            "* [SIM=0.480] Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query Example for Non-Matching Case\n",
        "results_with_scores = vector_store.similarity_search_with_score(\n",
        "    \"Who won 2024 Presidential Election in USA?\"\n",
        ")\n",
        "for res, score in results_with_scores:\n",
        "    print(f\"* [SIM={score:.3f}] {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erCNCpOEwm-W",
        "outputId": "15b21fe3-27d0-489b-e20b-5636fc7d815d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* [SIM=0.510] I had chocolate chip pancakes and scrambled eggs for breakfast this morning. [{'source': 'tweet'}]\n",
            "* [SIM=0.492] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees Fahrenheit. [{'source': 'news'}]\n",
            "* [SIM=0.480] Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# Initialize Google Generative AI LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")"
      ],
      "metadata": {
        "id": "GBlxofNlzheO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Function to Answer User Queries\n",
        "def answer_to_user(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Answers the user query by retrieving relevant documents and using LLM for generation.\n",
        "\n",
        "    Args:\n",
        "        query (str): User's query.\n",
        "\n",
        "    Returns:\n",
        "        str: Final answer generated by the LLM.\n",
        "    \"\"\"\n",
        "    # Perform Vector Search\n",
        "    vector_results = vector_store.similarity_search(query, k=2)\n",
        "    print(len(vector_results))\n",
        "\n",
        "    # Generate Answer Using LLM\n",
        "    final_answer = llm.invoke(f\"ANSWER THIS USER QUERY: {query}, Here are some reference to answer: {vector_results}\")\n",
        "\n",
        "    return final_answer"
      ],
      "metadata": {
        "id": "Efstm_hgzPFf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Query\n",
        "answer = answer_to_user(\"LangeChain provide abstractions to make working with LLMs easy\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HEfJHzozPCJ",
        "outputId": "3d7e4a31-d0d2-4abd-d276-27175cbab0c0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "HuHucgJIzO-e",
        "outputId": "2605d352-0c03-4d72-d625-04512c39b40c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"LangChain simplifies working with Large Language Models (LLMs).  The provided documents don't offer details on *how* LangChain achieves this simplification, but one document mentions someone is using LangChain in a new project.  The other document is irrelevant to the query.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VwMRJYWPzO7D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}